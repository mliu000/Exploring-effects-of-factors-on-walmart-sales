---
title: "group_project_proposal_p5"
format: html
editor: visual
due: Monday, Dec 9
---

## Stat 306 project group P5

```{r}
library(dplyr)
```

## Wrangling the data set

Here we wrangle the data set to determine how to perform the analysis. Make sure you have your working directory set to the folder where the "Walmart_Sales.csv" file is.

```{r}
# Set working directiry
data = read.csv("Walmart_Sales.csv") |> na.omit()

# Modify the contents of the data below
#############

# Change the temperature to celcius
data$Temperature = round((data$Temperature - 32)*(5/9), 2)

# Round the decimal places for the following columns
data$Fuel_Price = round(data$Fuel_Price, 2)
data$CPI = round(data$CPI, 2)
data$Weekly_Sales = round(data$Weekly_Sales, 0)

# Convert Holiday flag to factor (because it's categorical)
data$Holiday_Flag = factor(data$Holiday_Flag)

############

# We want to select the store that represents all the stores well. Therefore, we will select the store that has the median combined average weekly sales and variance of weekly sales. To do so, we will calculate the average and var of weekly sales for every store, then assign a ranking for both statistics, sum them up, then pick the store with the median sum of the ranks. 

# First, obtain the summary statistics for each store
data_summary = data |> group_by(Store) |> summarize(mean_weekly_sales = mean(Weekly_Sales), var_weekly_sales = var(Weekly_Sales)) |> mutate(rank_mean_weekly_sales = rank(mean_weekly_sales), rank_var_weekly_sales = rank(var_weekly_sales)) |> mutate(sum_of_ranks = rank_mean_weekly_sales + rank_var_weekly_sales)

# Then raange the rows based on the overall rank
data_summary = data_summary |> arrange(sum_of_ranks) 

# Extract the store number we choose (the store with median combined rank)
chosen_store = data_summary[round(nrow(data_summary)/2, 0), 1] |> pull()
print(chosen_store)

# We will use store 40 as our representative

# The boxplot illustrates the average weekly sales for each store, as well as the variance of the sales across its weeks. We can see that store 40 is reasonable
boxplot(data$Weekly_Sales~data$Store, main = "Side by Side boxplot of the weekly sales of all stores", xlab = "Weekly Sales", ylab = "Store Number")

ggplot(data = data, aes(x = factor(Store), y = Weekly_Sales)) +
  geom_boxplot() +
  labs(title = "Side by Side boxplot of the weekly sales of all stores", x = "Store Number", y = "Weekly Sales (USD)")
```

**Analyze the data from the Store we chose**

```{r}
# Get the data for the store we chose
store_40 = data |> filter(Store == chosen_store)
print(store_40)

```

The `echo: false` option disables the printing of code (only output is displayed).

**Descriptive Analysis and Data Analysis**

1.  Weekly_Sales and continuous variables

    The following plots show that there is positive linear relationship between these three continuous variables and Weekly Sales (though the trend is **not very** positive???)

```{r}
library(ggplot2)

# Weekly Sales vs. Temperature including Holiday Flag interaction
ggplot(data = store_40, aes(x = Temperature, y = Weekly_Sales, col = Holiday_Flag)) +
  geom_point() + 
  labs(title = "Weekly Sales vs. Temperature with Holiday Flag interaction", x = "Temperature (Â°C)", y = "Weekly Sales (USD)") +
  geom_smooth(method = "lm", se = FALSE)


# Weekly Sales vs. Fuel Price including Holiday Flag interaction
ggplot(data = store_40, aes(x = Fuel_Price, y = Weekly_Sales, col = Holiday_Flag)) +
  geom_point() + 
  labs(title = "Weekly Sales vs. Fuel_Price with Holiday Flag interaction", x = "Fuel Price (USD/gallon)", y = "Weekly Sales (USD)") +
  geom_smooth(method = "lm", se = FALSE)

# Weekly Sales vs. CPI
ggplot(data = store_40, aes(x = CPI, y = Weekly_Sales, col = Holiday_Flag)) +
  geom_point() + 
  labs(title = "Weekly Sales vs. CPI index with Holiday Flag interaction", x = "CPI index", y = "Weekly Sales (USD)") +
  geom_smooth(method = "lm", se = FALSE)

# Weekly Sales vs. Unemployment
ggplot(data = store_40, aes(x = Unemployment, y = Weekly_Sales, col = Holiday_Flag)) +
  geom_point() + 
  labs(title = "Weekly Sales vs. Unemployment Rate with Holiday Flag interaction", x = "Unemployment Rate", y = "Weekly Sales (USD)") +
  geom_smooth(method = "lm", se = FALSE)
```

2.  Weekly_Sales and categorical variable (Holiday Flag)

```{r}
ggplot(data = store_40, aes(x = Holiday_Flag, y = Weekly_Sales)) +
  geom_boxplot() +
  labs(title = "Weekly Sales vs whether or not any Holiday Exists within the week", x = "Holiday Flag", y = "Weekly Sales (USD)")
```

3.  Heatmap

    To assess the relationships between the different features in the dataset, a correlation matrix was computed for the continuous variables, including **Temperature**, **Fuel Price**, **CPI**, and **Weekly Sales**. The correlation matrix was then visualized using a heatmap to provide a clear understanding of the linear relationships between these variables. The heatmap offers a visual representation of the strength and direction of the correlations. Positive correlations are represented by cool colors (blues), while negative correlations are shown in warm colors (reds). Variables with strong correlations (close to 1 or -1) are indicated by more intense colors, while weak or no correlations (around 0) are shown by lighter shades.

```{r}
library(ggplot2)
library(ggcorrplot)
```

```{r}
correlation_matrix=cor(store_40[,c('Weekly_Sales','Temperature','Fuel_Price','CPI')])
ggcorrplot(correlation_matrix,lab=TRUE,colors=c("red","white","blue"))+
ggtitle("Correlation Matrix of Walmart Sales Data")
```

**\*Interaction term**

From the following plots. we can see that there is no need to include interaction terms, because although the regression line does seem to change a bit, the standard error is very high, indicating the likelihood that the difference in slope is caused by variability in the data

**Analysis of the plots**

After examining the boxplot above, we notice that Weekly sales tend to be higher on weeks that contained holidays. After examining the scatterplots, we see that there is almost no relationship between Fuel price and CPI index against weekly sales, and a weak linear relationship for Temperature and Unemployment rate. However, the holiday flag does alter the regression line a bit. Also, there seems to be quite a bit of heteroscedasticity for all the plots. However, that may not be the case we check the residual plot for our final multiple regression model.

**Checking the VIF**

Before doing model selection, we want to take the VIF of all the covariates, to see if there is any multicollinearity

```{r}
library(car)

model <- lm(Weekly_Sales ~ CPI + Fuel_Price + Temperature + Unemployment + Holiday_Flag, data = store_40)
vif_values <- vif(model)
# VIF with all the covariates
print(vif_values)

# VIF after removing CPI
model_cpi_removed <- lm(Weekly_Sales ~ + Fuel_Price + Temperature + Unemployment + Holiday_Flag, data = store_40)
vif_values_cpi_removed <- vif(model_cpi_removed)
# VIF with vif_values_cpi_removed the covariates
print(vif_values_cpi_removed)
```

After calculating the VIF for all covariates altogether, we see that CPI and Unemployment is highly correlated, meaning that the beta parameters will be very sensitive and have high error. We will remove CPI because based on the exploratory scatterplots above, it seems to have less correlation to the response.

After removing the CPI, the VIF's for the remaining covariates are well below 10, indicating little to no multicollinaerity

**Model Selection**

For the model selection, backwards selection will be performed. The covariates for the full model will be based on the one the ones kept from the VIF check above. Therefore, 3 continuous variables (fuel price, temperature, cpi, unemployment rate), the holiday flag categorical variable, and all interactions, which is 7 covariates, or 8 beta parameters in total. We will perform it at a 15% significance level (even keeping covariates that only slightly contribute to the model), dropping the term with the higher p-value that is above the significance level. We will also remove the 2 outliers, to avoid having a biased model selection result (p-values are higher than they are supposed to be due to the outliers).

```{r}
## First remove the outliers 
store_40 = store_40 |> filter(Weekly_Sales < 1500000)
## Perform the model selection

models = list()

# Start with full model with 7 covariates
models[[1]] = lm(Weekly_Sales ~ Temperature + Fuel_Price + Unemployment + Holiday_Flag + Temperature:Holiday_Flag + Fuel_Price:Holiday_Flag + Unemployment:Holiday_Flag, data = store_40)

# Second Model with Fuel_Price:Holiday_Flag interaction removed
models[[2]] = lm(Weekly_Sales ~ Temperature + Fuel_Price + Unemployment + Holiday_Flag + Temperature:Holiday_Flag + Unemployment:Holiday_Flag, data = store_40)

# Third Model with Temperature:Holiday_Flag interaction removed
models[[3]] = lm(Weekly_Sales ~ Temperature + Fuel_Price + Unemployment + Holiday_Flag + Unemployment:Holiday_Flag, data = store_40)

# Fourth Model with Unemployment:Holiday_Flag interaction removed
models[[4]] = lm(Weekly_Sales ~ Temperature + Fuel_Price + Unemployment + Holiday_Flag, data = store_40)

# Final Model with Unemployment:Holiday_Flag interaction removed
models[[5]] = lm(Weekly_Sales ~ Temperature + Unemployment + Holiday_Flag, data = store_40)


# Print out the summary of the final model and full model
summary(models[[5]])
```

**Variables removed in the Backward's selection process**

| Step | Variable Removed          | P-value |
|------|---------------------------|---------|
| 1    | Fuel_Price:Holiday_Flag   | 0.931   |
| 2    | Temperature:Holiday_Flag  | 0.850   |
| 3    | Unemployment:Holiday_Flag | 0.458   |
| 4    | Fuel_Price                | 0.244   |

Our final model involves keeping only 3 variables (that at least slightly contributes to the change in weekly sales). They are: Intercept, Termperature, Unemployment, and Holiday_Flag with p-values: \~0, \~0 and 0.113 respectively.

**Check whether or not the covariates chosen are the ones we expected**

```{r}
# Pretty much replotting the scatterplots above, but this time without the interaction (since they were removed in the backwards selection process)
# Weekly Sales vs. Temperature
ggplot(data = store_40, aes(x = Temperature, y = Weekly_Sales)) +
  geom_point() + 
  labs(title = "Weekly Sales vs. Temperature with Holiday Flag interaction", x = "Temperature (Â°C)", y = "Weekly Sales (USD)") +
  geom_smooth(method = "lm", se = FALSE)

# Weekly Sales vs. Unemployment
ggplot(data = store_40, aes(x = Unemployment, y = Weekly_Sales)) +
  geom_point() + 
  labs(title = "Weekly Sales vs. Unemployment Rate with Holiday Flag interaction", x = "Unemployment Rate", y = "Weekly Sales (USD)") +
  geom_smooth(method = "lm", se = FALSE)

ggplot(data = store_40, aes(x = Holiday_Flag, y = Weekly_Sales)) +
  geom_boxplot() +
  labs(title = "Weekly Sales vs whether or not any Holiday Exists within the week", x = "Holiday Flag", y = "Weekly Sales (USD)")
```

Also, after comparing the covariates chosen in the final model to the scatterplots above, we see that all the plots show somewhat of a correlation between the covariate and the response, which follows our intuition

**Examine the adequacy of the final model by comparing the R\^2 and adj R \^2 of the full model and the final simpler model**

| Model               | R\^2  | Adj R\^2 |
|---------------------|-------|----------|
| Full Model          | 0.147 | 0.089    |
| Final Simpler Model | 0.135 | 0.116    |

As noted above, the final model has an R\^2 that is slightly lower than the full model, but has a higher adj R \^2, which means its fewer covariates explain most of the variation in the response that is explained by the full model. Since the final model has fewer covariates, but is almost as powerful as the full model at explaining the variation in the response, the final model is the one to choose.

**Examine the fit of the final chosen model by creating a residual plot.**

Plot the residuals of the final model against the fitted values to check the satisfaction of the assumption of linearity and homoscedasticity.

```{r}
# Residual plot of the fitted values against the residuals
plot(models[[5]]$fitted.values, models[[5]]$residuals, main = "Residual plot for the  residuals of our\n final model against its fitted values", xlab = "Fitted values (Weekly sales in USD)", ylab = "Residual (Weekly sales in USD)")

abline(h = 0, col = "red", lwd = 2, lty = 2)
```

Result:

After examining the residual plot, the residuals in general are pretty patternless, with the exception of a few outliers, but the general appearance seems randomly scattered. Therefore the assumption of linearity is likely satisfied. In regards to the assumption of homoscedasticity, although the residuals seem to have slightly higher variance for lower fitted values, they are likely caused by random variability if our data set. However, ignoring some of the outliers on the right, and top of the plot, all other residuals seem to have constant variance, likely satisfying the assumption of constant variance/homoscedasticity.
